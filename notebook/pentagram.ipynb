{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f23fe4550a064ff18231fca0092a9113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c31fbcb4216e49bc8ad252f1e6b349ef",
              "IPY_MODEL_587b504018914d6690a1b69536d68415",
              "IPY_MODEL_2a690c6bf7f9415f85dfd93652d74602"
            ],
            "layout": "IPY_MODEL_143d99c882e948748083bd7c1ae6c8bf"
          }
        },
        "c31fbcb4216e49bc8ad252f1e6b349ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c035d308d9b4a659fc94d352e0e0c91",
            "placeholder": "​",
            "style": "IPY_MODEL_85bbd2bf388a482d95494d406c9f0e2c",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "587b504018914d6690a1b69536d68415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c37d64ab3144d33b1221aa678ddf5e4",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8acc166f5e0f4d929da58815fe787c82",
            "value": 7
          }
        },
        "2a690c6bf7f9415f85dfd93652d74602": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58cac9aaaa0b45dbbf478cefc63300af",
            "placeholder": "​",
            "style": "IPY_MODEL_6667b53bbeff443dbdd6c2f0521bf61a",
            "value": " 7/7 [00:01&lt;00:00,  3.95it/s]"
          }
        },
        "143d99c882e948748083bd7c1ae6c8bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c035d308d9b4a659fc94d352e0e0c91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85bbd2bf388a482d95494d406c9f0e2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c37d64ab3144d33b1221aa678ddf5e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8acc166f5e0f4d929da58815fe787c82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "58cac9aaaa0b45dbbf478cefc63300af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6667b53bbeff443dbdd6c2f0521bf61a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "983d1bcca7a04129bc4a4800aeb03a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_13018d6929c7438da26d8d90d891c9ef",
              "IPY_MODEL_efd7ba637623460baebc98aea6543bf4",
              "IPY_MODEL_b2d7bc850f1544f8987d3118e246f71f"
            ],
            "layout": "IPY_MODEL_93d18adb9e794c2cb7268f533263a460"
          }
        },
        "13018d6929c7438da26d8d90d891c9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c668dcbea5c4361881b693ad3690a8e",
            "placeholder": "​",
            "style": "IPY_MODEL_0bb0f0daa5ea4798bf2fd7ac6b087df9",
            "value": "100%"
          }
        },
        "efd7ba637623460baebc98aea6543bf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f0ac7d1b83b40c0bfe1e32627e123b1",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39813b1814a2474ba2e0c586d04deaaf",
            "value": 50
          }
        },
        "b2d7bc850f1544f8987d3118e246f71f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59b9a2b807db4d519881cd8d6644ae20",
            "placeholder": "​",
            "style": "IPY_MODEL_6f585041cd8c41a9be419d8177b658de",
            "value": " 50/50 [45:08&lt;00:00, 53.98s/it]"
          }
        },
        "93d18adb9e794c2cb7268f533263a460": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c668dcbea5c4361881b693ad3690a8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb0f0daa5ea4798bf2fd7ac6b087df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f0ac7d1b83b40c0bfe1e32627e123b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39813b1814a2474ba2e0c586d04deaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59b9a2b807db4d519881cd8d6644ae20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f585041cd8c41a9be419d8177b658de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pentagram\n",
        "\n",
        "**SOURCES**:\n",
        "\n",
        "- [Getting started with Modal](https://modal.com/docs/examples/hello_world)\n",
        "- [Building an Image Generation Pipeline on Modal](https://www.youtube.com/watch?v=sHSKArbiKmU)\n",
        "- [Run Stable Diffusion as a CLI, API and webUI](https://modal.com/docs/examples/text_to_image)\n",
        "- [Midjourney Examples](https://www.midjourney.com/explore?tab=top)\n",
        "- [NVIDIA GPU comparison](https://www.digitalocean.com/community/tutorials/h100_vs_other_gpus_choosing_the_right_gpu_for_your_machine_learning_workload)\n",
        "- [Modal Playground](https://modal.com/playground/get_started)\n",
        "- [Modal cold Start Guide](https://modal.com/docs/guide/cold-start)\n",
        "- [Image Generation Models](https://huggingface.co/models?pipeline_tag=text-to-image)\n",
        "- [Modal Web endpoints](https://modal.com/docs/guide/webhooks)\n",
        "\n",
        "## Objective\n",
        "For this project, you are tasked with building an Instagram clone, where instead of users uploading pictures themselves, they can generate images with text prompts. Instead of using existing image generation APIs, you will have to host an image generation model yourself on serverless GPUs and ensure low latency for a smooth user experience.\n",
        "\n",
        "Getting Started:\n",
        "\n",
        "Learn how Modal works here, along with the other resources provided above\n",
        "Set up the backend API using Modal that generates images from a text prompt\n",
        "Clone the GitHub repo here for the web app where users can generate images, and take a look at the TODOs in the codebase\n",
        "Project Requirements:\n",
        "\n",
        "Host an image generation model (e.g., Stable Diffusion) on serverless GPUs through Modal, ensuring low-latency performance for smooth user experience.\n",
        "Create a web app that allows users to generate images from text prompts, manage their creations, and interact socially through likes, comments, and sharing features.\n",
        "Incorporate intuitive UI/UX design, authentication, and efficient image management with prompt histories.\n",
        "Challenges:\n",
        "\n",
        "Ensuring the hosted image generation model operates within low-latency thresholds (<2 seconds) while handling multiple concurrent requests\n",
        "Managing the dynamic scaling of GPU resources to handle demand spikes without exceeding cost or causing performance bottlenecks.\n",
        "Add the ability to search for images semantically\n",
        "Prevent harmful or inappropriate content from being generated\n",
        "Build a recommendation system that creates personalized feeds for users, balancing new content discovery with user preferences"
      ],
      "metadata": {
        "id": "MCPYTZB1ZEHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Tips\n",
        "\n",
        "* Diffuser packages often require significant storage space and computational resources. For optimal performance, it's recommended to run them on platforms like Google Colab or virtual machines with sufficient storage and GPU capabilities."
      ],
      "metadata": {
        "id": "UHLaRbF5hhIb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "lh9mTwlpU1og"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation"
      ],
      "metadata": {
        "id": "eCFzw4ROTL7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install --upgrade modal \\\n",
        " diffusers \\\n",
        " requests \\\n",
        " torch \\\n",
        " fastapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6paRmxKz8WlA",
        "outputId": "236246ba-6489-4a6d-bfda-8b4fa5c6eaa1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting modal\n",
            "  Downloading modal-0.68.42-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.10/dist-packages (0.31.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Collecting fastapi\n",
            "  Downloading fastapi-0.115.6-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from modal) (3.11.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from modal) (2024.12.14)\n",
            "Requirement already satisfied: click>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from modal) (8.1.7)\n",
            "Collecting grpclib==0.4.7 (from modal)\n",
            "  Downloading grpclib-0.4.7.tar.gz (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf!=4.24.0,<6.0,>=3.19 in /usr/local/lib/python3.10/dist-packages (from modal) (4.25.5)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from modal) (13.9.4)\n",
            "Collecting synchronicity~=0.9.7 (from modal)\n",
            "  Downloading synchronicity-0.9.7-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from modal) (0.10.2)\n",
            "Requirement already satisfied: typer>=0.9 in /usr/local/lib/python3.10/dist-packages (from modal) (0.15.1)\n",
            "Collecting types-certifi (from modal)\n",
            "  Downloading types_certifi-2021.10.8.3-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting types-toml (from modal)\n",
            "  Downloading types_toml-0.10.8.20240310-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting watchfiles (from modal)\n",
            "  Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: typing-extensions~=4.6 in /usr/local/lib/python3.10/dist-packages (from modal) (4.12.2)\n",
            "Collecting h2<5,>=3.1.0 (from grpclib==0.4.7->modal)\n",
            "  Downloading h2-4.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: multidict in /usr/local/lib/python3.10/dist-packages (from grpclib==0.4.7->modal) (6.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.27.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers) (11.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Collecting starlette<0.42.0,>=0.40.0 (from fastapi)\n",
            "  Downloading starlette-0.41.3-py3-none-any.whl.metadata (6.0 kB)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from fastapi) (2.10.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.2->diffusers) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->modal) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->modal) (2.18.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.42.0,>=0.40.0->fastapi) (3.7.1)\n",
            "Collecting sigtools>=4.0.1 (from synchronicity~=0.9.7->modal)\n",
            "  Downloading sigtools-4.0.1-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.9->modal) (1.5.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.5.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->modal) (1.18.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.42.0,>=0.40.0->fastapi) (1.2.2)\n",
            "Collecting hyperframe<7,>=6.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n",
            "  Downloading hyperframe-6.0.1-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting hpack<5,>=4.0 (from h2<5,>=3.1.0->grpclib==0.4.7->modal)\n",
            "  Downloading hpack-4.0.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->modal) (0.1.2)\n",
            "Downloading modal-0.68.42-py3-none-any.whl (506 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m506.4/506.4 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.6-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading starlette-0.41.3-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading synchronicity-0.9.7-py3-none-any.whl (34 kB)\n",
            "Downloading types_certifi-2021.10.8.3-py3-none-any.whl (2.1 kB)\n",
            "Downloading types_toml-0.10.8.20240310-py3-none-any.whl (4.8 kB)\n",
            "Downloading watchfiles-1.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (443 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.8/443.8 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-4.1.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sigtools-4.0.1-py2.py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-4.0.0-py3-none-any.whl (32 kB)\n",
            "Downloading hyperframe-6.0.1-py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: grpclib\n",
            "  Building wheel for grpclib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grpclib: filename=grpclib-0.4.7-py3-none-any.whl size=76218 sha256=4553c34c71cbdc09a5cde9a4b41acfc30f70e19b944e425a4e2622024afc225b\n",
            "  Stored in directory: /root/.cache/pip/wheels/05/c0/1c/3d807409d0c67efeab2949832ba409205b1b6fe03f739ae4c1\n",
            "Successfully built grpclib\n",
            "Installing collected packages: types-certifi, types-toml, sigtools, hyperframe, hpack, watchfiles, synchronicity, starlette, h2, grpclib, fastapi, modal\n",
            "Successfully installed fastapi-0.115.6 grpclib-0.4.7 h2-4.1.0 hpack-4.0.0 hyperframe-6.0.1 modal-0.68.42 sigtools-4.0.1 starlette-0.41.3 synchronicity-0.9.7 types-certifi-2021.10.8.3 types-toml-0.10.8.20240310 watchfiles-1.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check Versions"
      ],
      "metadata": {
        "id": "Bx-MPwoIRCJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python --version\n",
        "!nvcc --version   # CUDA version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okZHR89qQ1H-",
        "outputId": "ae66e2b8-2e5a-456a-9fa1-b981f2e8cdc7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2023 NVIDIA Corporation\n",
            "Built on Tue_Aug_15_22:02:13_PDT_2023\n",
            "Cuda compilation tools, release 12.2, V12.2.140\n",
            "Build cuda_12.2.r12.2/compiler.33191640_0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check GPU\n",
        "\n",
        "To use the GPU on Colab, you have to choose T4 for $ 1.44 per hour"
      ],
      "metadata": {
        "id": "pWg6yoxxRGBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(\"torch versions: \", torch.__version__)\n",
        "print(\"Cuda availability: \", torch.cuda.is_available())\n",
        "print(\"CUDA device count:\", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-ZGS3V_RJ71",
        "outputId": "4754fcc2-1020-4104-c329-7ebdf8ddeb18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch versions:  2.5.1+cu121\n",
            "Cuda availability:  False\n",
            "CUDA device count: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Environment Variables"
      ],
      "metadata": {
        "id": "vHEFnkCM8a_F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Colab\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['BFL_API_KEY'] = userdata.get('BFL_API_KEY')\n",
        "\n",
        "hf_token = os.environ['HF_TOKEN']\n",
        "bfl_api_key = os.environ['BFL_API_KEY']"
      ],
      "metadata": {
        "id": "DAvdmPL_8bIo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model List"
      ],
      "metadata": {
        "id": "dQKzdSfPJkiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bf_model = \"black-forest-labs/FLUX.1-dev\"\n",
        "sd_model = \"stabilityai/stable-diffusion-3.5-large-turbo\"\n",
        "sdx1_model = \"stabilityai/sdxl-turbo\"\n",
        "\n",
        "adamo_model = \"adamo1139/stable-diffusion-3.5-large-turbo-ungated\"\n",
        "adamo_model_id = \"9ad870ac0b0e5e48ced156bb02f85d324b7275d2\""
      ],
      "metadata": {
        "id": "XqZ-rMHrJm-j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hugging Face Model: Flux\n",
        "**SOURCES:**\n",
        "* [HF: Black-forest-labs](https://huggingface.co/black-forest-labs/FLUX.1-dev)\n",
        "* [Diffusers for MacOS](https://huggingface.co/docs/diffusers/optimization/mps)\n",
        "\n",
        "To use the diffusers, you need to get access token from hugging face:\n",
        "* Go to Setting > Access Token > {user access token name} > edit permission > \"Read access to contents of all public gated repos you can access\"\n",
        "* Go to the terminal and enter:\n",
        "```terminal\n",
        "huggingface-cli login`\n",
        "```\n",
        "\n",
        "This command will prompt you for a token. Copy-paste yours and press Enter. Then, you’ll be asked if the token should also be saved as a git credential. I chose 'NO'. Finally, it will call the Hub to check that your token is valid and save it locally.\n",
        "\n",
        "**Problem:** _DiffusionPipeline keeps crashing_\n",
        "Error says \"Your session crashed after using all available RAM\"\n",
        "\n",
        "**Solution:** Use PyTorch2.0 and `pipe.to(\"mps\")`\n"
      ],
      "metadata": {
        "id": "fiehVdGTGcjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import (DiffusionPipeline, FluxPipeline)\n",
        "import torch\n",
        "\n",
        "pipe = DiffusionPipeline.from_pretrained(sdx1_model)\n",
        "\n",
        "# If you are running this on Apples M1/M2 chips, Use 'to.mps'\n",
        "# pipe = pipe.to(\"mps\")\n",
        "\n",
        "# ENABLE_MODEL_CPU_OFFLOAD: save some VRAM by offloading the model to CPU. Remove this if you have enough GPU power\n",
        "# pipe.enable_model_cpu_offload\n",
        "\n",
        "# Recommended if your computer has < 64 GB of RAM\n",
        "pipe.enable_attention_slicing()\n",
        "\n",
        "prompt = \"A cat holding a sign that says hello world\"\n",
        "image = pipe(\n",
        "    prompt,\n",
        "    height=1024,\n",
        "    width=1024,\n",
        "    guidance_scale=3.5,\n",
        "    num_inference_steps=50,\n",
        "    max_sequence_length=512,\n",
        ").images[0]\n",
        "image.save(\"flux-dev.png\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "f23fe4550a064ff18231fca0092a9113",
            "c31fbcb4216e49bc8ad252f1e6b349ef",
            "587b504018914d6690a1b69536d68415",
            "2a690c6bf7f9415f85dfd93652d74602",
            "143d99c882e948748083bd7c1ae6c8bf",
            "5c035d308d9b4a659fc94d352e0e0c91",
            "85bbd2bf388a482d95494d406c9f0e2c",
            "2c37d64ab3144d33b1221aa678ddf5e4",
            "8acc166f5e0f4d929da58815fe787c82",
            "58cac9aaaa0b45dbbf478cefc63300af",
            "6667b53bbeff443dbdd6c2f0521bf61a",
            "983d1bcca7a04129bc4a4800aeb03a09",
            "13018d6929c7438da26d8d90d891c9ef",
            "efd7ba637623460baebc98aea6543bf4",
            "b2d7bc850f1544f8987d3118e246f71f",
            "93d18adb9e794c2cb7268f533263a460",
            "1c668dcbea5c4361881b693ad3690a8e",
            "0bb0f0daa5ea4798bf2fd7ac6b087df9",
            "7f0ac7d1b83b40c0bfe1e32627e123b1",
            "39813b1814a2474ba2e0c586d04deaaf",
            "59b9a2b807db4d519881cd8d6644ae20",
            "6f585041cd8c41a9be419d8177b658de"
          ]
        },
        "id": "lK8mTUCUGfZX",
        "outputId": "643a38e0-6493-457a-ab13-17823ef97d68"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f23fe4550a064ff18231fca0092a9113"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "983d1bcca7a04129bc4a4800aeb03a09"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modal"
      ],
      "metadata": {
        "id": "DKzswNk88yAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Modal and setting up\n",
        "**SOURCE**:\n",
        "* [Modal getting started](https://modal.com/docs/examples/hello_world)"
      ],
      "metadata": {
        "id": "RRpf6GjT82Z0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import modal"
      ],
      "metadata": {
        "id": "ZBe0_SgH8ySp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%python -m modal setup"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eB9Pf5AC9Dm4",
        "outputId": "f6c7869e-92e1-49f9-937f-ad09a3120262"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dir(modal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-NYOtvTdySS",
        "outputId": "7a575690-9f05-483c-9923-6922f89ee374"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['App',\n",
              " 'Client',\n",
              " 'CloudBucketMount',\n",
              " 'Cls',\n",
              " 'Cron',\n",
              " 'Dict',\n",
              " 'Error',\n",
              " 'FilePatternMatcher',\n",
              " 'Function',\n",
              " 'Image',\n",
              " 'Mount',\n",
              " 'NetworkFileSystem',\n",
              " 'Period',\n",
              " 'Proxy',\n",
              " 'Queue',\n",
              " 'Retries',\n",
              " 'Sandbox',\n",
              " 'SchedulerPlacement',\n",
              " 'Secret',\n",
              " 'Stub',\n",
              " 'Tunnel',\n",
              " 'Volume',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " '_ipython',\n",
              " '_location',\n",
              " '_pty',\n",
              " '_resolver',\n",
              " '_resources',\n",
              " '_runtime',\n",
              " '_serialization',\n",
              " '_traceback',\n",
              " '_tunnel',\n",
              " '_utils',\n",
              " '_vendor',\n",
              " 'app',\n",
              " 'asgi_app',\n",
              " 'batched',\n",
              " 'build',\n",
              " 'call_graph',\n",
              " 'client',\n",
              " 'cloud_bucket_mount',\n",
              " 'cls',\n",
              " 'config',\n",
              " 'container_process',\n",
              " 'current_function_call_id',\n",
              " 'current_input_id',\n",
              " 'dict',\n",
              " 'enable_output',\n",
              " 'enter',\n",
              " 'environments',\n",
              " 'exception',\n",
              " 'exit',\n",
              " 'file_io',\n",
              " 'file_pattern_matcher',\n",
              " 'forward',\n",
              " 'functions',\n",
              " 'gpu',\n",
              " 'image',\n",
              " 'interact',\n",
              " 'io_streams',\n",
              " 'is_local',\n",
              " 'method',\n",
              " 'mount',\n",
              " 'network_file_system',\n",
              " 'object',\n",
              " 'output',\n",
              " 'parallel_map',\n",
              " 'parameter',\n",
              " 'partial_function',\n",
              " 'proxy',\n",
              " 'queue',\n",
              " 'retries',\n",
              " 'running_app',\n",
              " 'sandbox',\n",
              " 'schedule',\n",
              " 'scheduler_placement',\n",
              " 'secret',\n",
              " 'stream_type',\n",
              " 'sys',\n",
              " 'volume',\n",
              " 'web_endpoint',\n",
              " 'web_server',\n",
              " 'wsgi_app']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run Stable Diffusion as a CLI, API, and web UI\n",
        "\n",
        "https://modal.com/docs/examples/text_to_image\n",
        "\n",
        "This example shows how to run Stable Diffusion 3.5 Large Turbo on Modal to generate images from your local command line, via an API, and as a web UI.\n",
        "\n",
        "Inference takes about one minute to cold start, at which point images are generated at a rate of one image every 1-2 seconds for batch sizes between one and 16."
      ],
      "metadata": {
        "id": "cSNhlNQTZSg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Setup"
      ],
      "metadata": {
        "id": "TfOgDrvKb2iJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dir(modal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_WBejnndmxB",
        "outputId": "be06ecbc-b3e4-4715-a59b-6c59a29d6893"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['App',\n",
              " 'Client',\n",
              " 'CloudBucketMount',\n",
              " 'Cls',\n",
              " 'Cron',\n",
              " 'Dict',\n",
              " 'Error',\n",
              " 'FilePatternMatcher',\n",
              " 'Function',\n",
              " 'Image',\n",
              " 'Mount',\n",
              " 'NetworkFileSystem',\n",
              " 'Period',\n",
              " 'Proxy',\n",
              " 'Queue',\n",
              " 'Retries',\n",
              " 'Sandbox',\n",
              " 'SchedulerPlacement',\n",
              " 'Secret',\n",
              " 'Stub',\n",
              " 'Tunnel',\n",
              " 'Volume',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '__version__',\n",
              " '_ipython',\n",
              " '_location',\n",
              " '_pty',\n",
              " '_resolver',\n",
              " '_resources',\n",
              " '_runtime',\n",
              " '_serialization',\n",
              " '_traceback',\n",
              " '_tunnel',\n",
              " '_utils',\n",
              " '_vendor',\n",
              " 'app',\n",
              " 'asgi_app',\n",
              " 'batched',\n",
              " 'build',\n",
              " 'call_graph',\n",
              " 'client',\n",
              " 'cloud_bucket_mount',\n",
              " 'cls',\n",
              " 'config',\n",
              " 'container_process',\n",
              " 'current_function_call_id',\n",
              " 'current_input_id',\n",
              " 'dict',\n",
              " 'enable_output',\n",
              " 'enter',\n",
              " 'environments',\n",
              " 'exception',\n",
              " 'exit',\n",
              " 'file_io',\n",
              " 'file_pattern_matcher',\n",
              " 'forward',\n",
              " 'functions',\n",
              " 'gpu',\n",
              " 'image',\n",
              " 'interact',\n",
              " 'io_streams',\n",
              " 'is_local',\n",
              " 'method',\n",
              " 'mount',\n",
              " 'network_file_system',\n",
              " 'object',\n",
              " 'output',\n",
              " 'parallel_map',\n",
              " 'parameter',\n",
              " 'partial_function',\n",
              " 'proxy',\n",
              " 'queue',\n",
              " 'retries',\n",
              " 'running_app',\n",
              " 'sandbox',\n",
              " 'schedule',\n",
              " 'scheduler_placement',\n",
              " 'secret',\n",
              " 'stream_type',\n",
              " 'sys',\n",
              " 'volume',\n",
              " 'web_endpoint',\n",
              " 'web_server',\n",
              " 'wsgi_app']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Webpoints\n",
        "[SOURCE](https://github.com/modal-labs/modal-examples/blob/main/09_job_queues/doc_ocr_webapp.py)\n",
        "\n",
        "cmd: [\"modal\", \"serve\", \"07_web_endpoints/basic_web.py\"]\n",
        "\n",
        "\n",
        "### Hello world wide web!\n",
        "\n",
        "Modal makes it easy to turn your Python functions into serverless web services:\n",
        "access them via a browser or call them from any client that speaks HTTP, all\n",
        "without having to worry about setting up servers or managing infrastructure.\n",
        "\n",
        "This tutorial shows the path with the shortest [\"time to 200\"](https://shkspr.mobi/blog/2021/05/whats-your-apis-time-to-200/):\n",
        "[`modal.web_endpoint`](https://modal.com/docs/reference/modal.web_endpoint).\n",
        "\n",
        "On Modal, web endpoints have all the superpowers of Modal Functions:\n",
        "they can be [accelerated with GPUs](https://modal.com/docs/guide/gpu),\n",
        "they can access [Secrets](https://modal.com/docs/guide/secrets) or [Volumes](https://modal.com/docs/guide/volumes),\n",
        "and they [automatically scale](https://modal.com/docs/guide/cold-start) to handle more traffic.\n",
        "\n",
        "\n",
        "Under the hood, we use the [FastAPI library](https://fastapi.tiangolo.com/),\n",
        "which has [high-quality documentation](https://fastapi.tiangolo.com/tutorial/),\n",
        "linked throughout this tutorial.\n"
      ],
      "metadata": {
        "id": "Coi276QHKgOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Turn a Modal Function into an endpoint with a single decorator\n",
        "\n",
        "Modal Functions are already accessible remotely -- when you add the `@app.function` decorator to a Python function\n",
        "and run `modal deploy`, you make it possible for your [other Python functions to call it](https://modal.com/docs/guide/trigger-deployed-functions).\n",
        "\n",
        "That's great, but it's not much help if you want to share what you've written with someone running code in a different language --\n",
        "or not running code at all!\n",
        "\n",
        "And that's where most of the power of the Internet comes from: sharing information and functionality across different computer systems.\n",
        "\n",
        "So we provide the `web_endpoint` decorator to wrap your Modal Functions in the lingua franca of the web: HTTP.\n",
        "Here's what that looks like:\n",
        "\n",
        "```\n",
        "import modal\n",
        "\n",
        "image = modal.Image.debian_slim().pip_install(\"fastapi[standard]\")\n",
        "app = modal.App(name=\"example-lifecycle-web\", image=image)\n",
        "\n",
        "\n",
        "@app.function()\n",
        "@modal.web_endpoint(\n",
        "    docs=True  # adds interactive documentation in the browser\n",
        ")\n",
        "def hello():\n",
        "    return \"Hello world!\"\n",
        "```\n",
        "\n",
        "You can turn this function into a web endpoint by running `modal serve basic_web.py`.\n",
        "In the output, you should see a URL that ends with `hello-dev.modal.run`.\n",
        "If you navigate to this URL, you should see the `\"Hello world!\"` message appear in your browser.\n",
        "\n",
        "You can also find interactive documentation, powered by OpenAPI and Swagger,\n",
        "if you add `/docs` to the end of the URL.\n",
        "From this documentation, you can interact with your endpoint, sending HTTP requests and receiving HTTP responses.\n",
        "For more details, see the [FastAPI documentation](https://fastapi.tiangolo.com/features/#automatic-docs).\n",
        "\n",
        "By running the endpoint with `modal serve`, you created a temporary endpoint that will disappear if you interrupt your terminal.\n",
        "These temporary endpoints are great for debugging -- when you save a change to any of your dependent files, the endpoint will redeploy.\n",
        "Try changing the message to something else, hitting save, and then hitting refresh in your browser or re-sending\n",
        "the request from `/docs` or the command line. You should see the new message, along with logs in your terminal showing the redeploy and the request.\n",
        "\n",
        "When you're ready to deploy this endpoint permanently, run `modal deploy basic_web.py`.\n",
        "Now, your function will be available even when you've closed your terminal or turned off your computer."
      ],
      "metadata": {
        "id": "xIXQKE8SOa6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Send data to a web endpoint\n",
        "\n",
        "The web endpoint above was a bit silly: it always returns the same message.\n",
        "\n",
        "Most endpoints need an input to be useful. There are two ways to send data to a web endpoint:\n",
        "- in the URL as a [query parameter](#sending-data-in-query-parameters)\n",
        "- in the [body of the request](#sending-data-in-the-request-body) as JSON\n"
      ],
      "metadata": {
        "id": "2Zr29gZbOmOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sending data in query parameters\n",
        "\n",
        "By default, your function's arguments are treated as query parameters:\n",
        "they are extracted from the end of the URL, where they should be added in the form\n",
        "`?arg1=foo&arg2=bar`.\n",
        "\n",
        "From the Python side, there's hardly anything to do:\n",
        "```\n",
        "@app.function()\n",
        "@modal.web_endpoint(docs=True)\n",
        "def greet(user: str) -> str:\n",
        "    return f\"Hello {user}!\"\n",
        "```\n",
        "\n",
        "If you are already running `modal serve basic_web.py`, this endpoint will be available at a URL, printed in your terminal, that ends with `greet-dev.modal.run`.\n",
        "\n",
        "We provide Python type-hints to get type information in the docs and\n",
        "[automatic validation](https://fastapi.tiangolo.com/tutorial/query-params-str-validations/).\n",
        "For example, if you navigate directly to the URL for `greet`, you will get a detailed error message\n",
        "indicating that the `user` parameter is missing. Navigate instead to `/docs` to see how to invoke the endpoint properly.\n",
        "\n",
        "You can read more about query parameters in the [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/query-params/)."
      ],
      "metadata": {
        "id": "vB1eC316OuxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Sending data in the request body\n",
        "\n",
        "For larger and more complex data, it is generally preferrable to send data in the body of the HTTP request.\n",
        "This body is formatted as [JSON](https://developer.mozilla.org/en-US/docs/Learn/JavaScript/Objects/JSON),\n",
        "the most common data interchange format on the web.\n",
        "\n",
        "To set up an endpoint that accepts JSON data, add an argument with a `dict` type-hint to your function.\n",
        "This argument will be populated with the data sent in the request body.\n",
        "\n",
        "```\n",
        "@app.function()\n",
        "@modal.web_endpoint(method=\"POST\", docs=True)\n",
        "def goodbye(data: dict) -> str:\n",
        "    name = data.get(\"name\") or \"world\"\n",
        "    return f\"Goodbye {name}!\"\n",
        "```\n",
        "\n",
        "Note that we gave a value of `\"POST\"` for the `method` argument here.\n",
        "This argument defines the HTTP request method that the endpoint will respond to,\n",
        "and it defaults to `\"GET\"`.\n",
        "If you head to the URL for the `goodbye` endpoint in your browser,\n",
        "you will get a 405 Method Not Allowed error, because browsers only send GET requests by default.\n",
        "While this is technically a separate concern from query parameters versus request bodies\n",
        "and you can define an endpoint that accepts GET requests and uses data from the body,\n",
        "it is [considered bad form](https://stackoverflow.com/a/983458).\n",
        "\n",
        "Navigate to `/docs` for more on how to invoke the endpoint properly.\n",
        "You will need to send a POST request with a JSON body containing a `name` key.\n",
        "To get the same typing and validation benefits as with query parameters,\n",
        "use a [Pydantic model](https://fastapi.tiangolo.com/tutorial/body/)\n",
        "for this argument.\n",
        "\n",
        "You can read more about request bodies in the [FastAPI documentation](https://fastapi.tiangolo.com/tutorial/body/)."
      ],
      "metadata": {
        "id": "hMgUAWhsO0wq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Handle expensive startup with `modal.Cls`\n",
        "\n",
        "Sometimes your endpoint needs to do something before it can handle its first request,\n",
        "like get a value from a database or set the value of a variable.\n",
        "If that step is expensive, like [loading a large ML model](https://modal.com/docs/guide/model-weights),\n",
        "it'd be a shame to have to do it every time a request comes in!\n",
        "\n",
        "Web endpoints can be methods on a [`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-functions-and-parameters).\n",
        "Note that they don't need the [`modal.method`](https://modal.com/docs/reference/modal.method) decorator.\n",
        "\n",
        "This example will only set the `start_time` instance variable once, on container startup.\n",
        "\n",
        "```\n",
        "@app.cls()\n",
        "class WebApp:\n",
        "    @modal.enter()\n",
        "    def startup(self):\n",
        "        from datetime import datetime, timezone\n",
        "\n",
        "        print(\"🏁 Starting up!\")\n",
        "        self.start_time = datetime.now(timezone.utc)\n",
        "\n",
        "    @modal.web_endpoint(docs=True)\n",
        "    def web(self):\n",
        "        from datetime import datetime, timezone\n",
        "\n",
        "        current_time = datetime.now(timezone.utc)\n",
        "        return {\"start_time\": self.start_time, \"current_time\": current_time}\n",
        "```\n"
      ],
      "metadata": {
        "id": "Pl311qVCO77V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What next?\n",
        "\n",
        "Modal's `web_endpoint` decorator is opinionated and designed for relatively simple web applications --\n",
        "one or a few independent Python functions that you want to expose to the web.\n",
        "\n",
        "Three additional decorators allow you to serve more complex web applications with greater control:\n",
        "- [`asgi_app`](https://modal.com/docs/guide/webhooks#asgi) to serve applications compliant with the ASGI standard,\n",
        "like [FastAPI](https://fastapi.tiangolo.com/)\n",
        "- [`wsgi_app`](https://modal.com/docs/guide/webhooks#wsgi) to serve applications compliant with the WSGI standard,\n",
        "like [Flask](https://flask.palletsprojects.com/)\n",
        "- [`web_server`](https://modal.com/docs/guide/webhooks#non-asgi-web-servers) to serve any application that listens on a port"
      ],
      "metadata": {
        "id": "hBCD4eP8OE9J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Flux fast with torch.compile\n",
        "\n",
        "[source](https://modal.com/docs/examples/flux)\n"
      ],
      "metadata": {
        "id": "ZPSOwY4wXEHH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing SD3.5 large turbo inference on Modal\n",
        "\n",
        "We wrap inference in a Modal Cls that ensures models are downloaded when we `build` our container image (just like our dependencies) and that models are loaded and then moved to the GPU when a new container starts.\n",
        "\n",
        "The run function just wraps a `diffusers` pipeline. It sends the output image back to the client as bytes.\n",
        "\n",
        "We also include a web wrapper that makes it possible to trigger inference via an API call. See the /docs route of the URL ending in inference-web.modal.run that appears when you deploy the app for details.The Inference class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically.\n",
        "\n",
        "The `Inference` class will serve multiple users from its own auto-scaling pool of warm GPU containers automatically."
      ],
      "metadata": {
        "id": "YZelA23FcLuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#================================================================================================\n",
        "#\n",
        "# BASIC SETUP\n",
        "#\n",
        "#================================================================================================\n",
        "\n",
        "from io import BytesIO\n",
        "import random\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import modal\n",
        "\n",
        "# Running Flux fast\n",
        "# We’ll make use of the full CUDA toolkit in this example, so we’ll build our container image off of the nvidia/cuda base.\n",
        "cuda_version = \"12.4.0\"  # should be no greater than host CUDA version\n",
        "flavor = \"devel\"  # includes full CUDA toolkit\n",
        "operating_sys = \"ubuntu22.04\"\n",
        "tag = f\"{cuda_version}-{flavor}-{operating_sys}\"\n",
        "diffusers_commit_sha = \"81cf3b2f155f1de322079af28f625349ee21ec6b\" # original sha\n",
        "diffusers_commit_sha = \"9c0e20de61a6e0adcec706564cee739520c1d2f4\"\n",
        "\n",
        "cuda_dev_image = modal.Image.from_registry(\n",
        "    f\"nvidia/cuda:{tag}\", add_python=\"3.12\"\n",
        ").entrypoint([])\n",
        "\n",
        "flux_image = (\n",
        "    cuda_dev_image.apt_install(\n",
        "        \"git\",\n",
        "        \"libglib2.0-0\",\n",
        "        \"libsm6\",\n",
        "        \"libxrender1\",\n",
        "        \"libxext6\",\n",
        "        \"ffmpeg\",\n",
        "        \"libgl1\",\n",
        "    )\n",
        "    .pip_install(\n",
        "        \"invisible_watermark==0.2.0\",\n",
        "        \"transformers==4.44.0\",\n",
        "        \"huggingface_hub[hf_transfer]==0.26.2\",\n",
        "        \"accelerate==0.33.0\",\n",
        "        \"safetensors==0.4.4\",\n",
        "        \"sentencepiece==0.2.0\",\n",
        "        \"torch==2.5.1\",\n",
        "        f\"git+https://github.com/huggingface/diffusers.git@{diffusers_commit_sha}\",\n",
        "        \"numpy<2\",\n",
        "        \"accelerate==0.33.0\",\n",
        "        # \"diffusers==0.31.0\",\n",
        "        \"fastapi[standard]==0.115.4\",\n",
        "        \"torchvision==0.20.1\",\n",
        "    )\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})\n",
        ")\n",
        "\n",
        "# Torch compilation needs to be re-executed when each new container starts,\n",
        "# So we turn on some extra caching to reduce compile times for later containers.\n",
        "flux_image = flux_image.env(\n",
        "    {\"TORCHINDUCTOR_CACHE_DIR\": \"/root/.inductor-cache\"}\n",
        ").env({\"TORCHINDUCTOR_FX_GRAPH_CACHE\": \"1\"})\n",
        "\n",
        "\n",
        "basic_image = (\n",
        "    modal.Image.debian_slim(python_version=\"3.12\")\n",
        "    .pip_install(\n",
        "        \"accelerate==0.33.0\",\n",
        "        \"diffusers==0.31.0\",\n",
        "        \"fastapi[standard]==0.115.4\",\n",
        "        \"huggingface-hub[hf_transfer]==0.25.2\",\n",
        "        \"sentencepiece==0.2.0\",\n",
        "        \"torch==2.5.1\",\n",
        "        \"torchvision==0.20.1\",\n",
        "        \"transformers~=4.44.0\",\n",
        "    )\n",
        "    .env({\"HF_HUB_ENABLE_HF_TRANSFER\": \"1\"})  # faster downloads\n",
        ")\n",
        "\n",
        "# Switch between BASIC or FLUX\n",
        "image = flux_image\n",
        "\n",
        "# Creates the app. All Modal programs need an App — an object that acts as a recipe for the application. Let’s give it a friendly name.\n",
        "app = modal.App(\"pentagram-app\")\n",
        "\n",
        "# The `image.imports()` lets us conditionally import in the global scope.\n",
        "# This is needed because we might have the dependencies installed locally,\n",
        "# but we know they are installed inside the custom image.\n",
        "\n",
        "with image.imports():\n",
        "    from diffusers import (FluxPipeline, StableDiffusion3Pipeline, DiffusionPipeline)\n",
        "    import torch\n",
        "    from fastapi import Response\n",
        "\n",
        "\n",
        "#================================================================================================\n",
        "#\n",
        "# AVAILABLE DIFFUSION MODELS and DEFAULT PARAMS\n",
        "#\n",
        "#================================================================================================\n",
        "\n",
        "MINUTES = 60 #seconds\n",
        "VARIANT = \"dev\"  # \"schnell\" or \"dev\", but note [dev] requires you to accept terms and conditions on HF\n",
        "NUM_INFERENCE_STEPS = 50  # use ~50 for [dev], smaller (~4) for [schnell]\n",
        "\n",
        "bf_model = f\"black-forest-labs/FLUX.1-{VARIANT}\"\n",
        "sd_model = \"stabilityai/stable-diffusion-3.5-large-turbo\"\n",
        "sdx1_model = \"stabilityai/sdxl-turbo\"\n",
        "\n",
        "adamo_model = \"adamo1139/stable-diffusion-3.5-large-turbo-ungated\"\n",
        "adamo_revision_id = \"9ad870ac0b0e5e48ced156bb02f85d324b7275d2\"\n",
        "\n",
        "#================================================================================================\n",
        "#\n",
        "# MODEL CLASS\n",
        "#\n",
        "#================================================================================================\n",
        "@app.cls(\n",
        "    image=image,\n",
        "    gpu=\"a10g\",     # Cheapest GPU\n",
        "    container_idle_timeout=20 * MINUTES,\n",
        "    timeout=60 * MINUTES,  # leave plenty of time for compilation\n",
        "    volumes={  # add Volumes to store serializable compilation artifacts, see section on torch.compile below\n",
        "    \"/root/.nv\": modal.Volume.from_name(\"nv-cache\", create_if_missing=True),\n",
        "    \"/root/.triton\": modal.Volume.from_name(\n",
        "        \"triton-cache\", create_if_missing=True\n",
        "    ),\n",
        "    \"/root/.inductor-cache\": modal.Volume.from_name(\n",
        "        \"inductor-cache\", create_if_missing=True\n",
        "    ),\n",
        "},\n",
        ")\n",
        "class Model:\n",
        "    compile: int = (  # see section on torch.compile below for details\n",
        "      modal.parameter(default=0)\n",
        "    )\n",
        "\n",
        "    def setup_model(self)\n",
        "    @modal.build()\n",
        "    @modal.enter()\n",
        "    def __init__(self):\n",
        "        \"\"\"\n",
        "        Initialize our diffusion model\n",
        "        \"\"\"\n",
        "        sdx1_model = \"stabilityai/sdxl-turbo\"\n",
        "        self.diffuser = StableDiffusion3Pipeline.from_pretrained(\n",
        "            sdx1_model,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )\n",
        "\n",
        "    @modal.enter()\n",
        "    def move_to_gpu(self):\n",
        "        self.pipe.to(\"cuda\")\n",
        "\n",
        "    def generateImage(self, prompt, batch_size:int = 4):\n",
        "        imageOutput = self.diffuser(\n",
        "            prompt,\n",
        "            height=1024,\n",
        "            width=1024,\n",
        "            guidance_scale=3.5,\n",
        "            num_inference_steps=NUM_INFERENCE_STEPS,\n",
        "            max_sequence_length=512,\n",
        "            num_images_per_prompt=batch_size,  # outputting multiple images per prompt is much cheaper than separate calls\n",
        "            num_inference_steps=4,  # turbo is tuned to run in four steps\n",
        "            guidance_scale=3.5,  # turbo doesn't use CFG\n",
        "            max_sequence_length=512,  # T5-XXL text encoder supports longer sequences, more complex prompts\n",
        "        ).images\n",
        "        return imageOutput\n",
        "\n",
        "    def run(\n",
        "        self, prompt: str, batch_size: int = 4, seed: int = None\n",
        "    ) -> list[bytes]:\n",
        "        seed = seed if seed is not None else random.randint(0, 2**32 - 1)\n",
        "        print(\"seeding RNG with\", seed)\n",
        "        torch.manual_seed(seed)\n",
        "\n",
        "        images = self.generateImage(prompt, batch_size)\n",
        "\n",
        "        buffer = []\n",
        "        for image in images:\n",
        "            with BytesIO() as buf:\n",
        "                image.save(buf, format=\"PNG\")\n",
        "                buffer.append(buf.getvalue())\n",
        "        torch.cuda.empty_cache()  # reduce fragmentation\n",
        "        return buffer\n",
        "\n",
        "    # Sometimes your endpoint needs to do something before it can handle its first request,\n",
        "    # like get a value from a database or set the value of a variable.\n",
        "    # If that step is expensive, like [loading a large ML model](https://modal.com/docs/guide/model-weights),\n",
        "    # it'd be a shame to have to do it every time a request comes in!\n",
        "\n",
        "    # Web endpoints can be methods on a [`modal.Cls`](https://modal.com/docs/guide/lifecycle-functions#container-lifecycle-functions-and-parameters).\n",
        "    # Note that they don't need the [`modal.method`](https://modal.com/docs/reference/modal.method) decorator.\n",
        "    @modal.web_endpoint(docs=True)\n",
        "    def web(self, prompt: str, seed: int = None):\n",
        "        return Response(\n",
        "            content=self.run.local(  # run in the same container\n",
        "                prompt, batch_size=1, seed=seed\n",
        "            )[0],\n",
        "            media_type=\"image/png\",\n",
        "        )\n",
        "\n",
        "\n",
        "#================================================================================================\n",
        "#\n",
        "# MODAL APP'S ENTRYPOINT\n",
        "#\n",
        "# This will trigger the run locally. The first time we run this,\n",
        "# it will take 1-2 min. When we run this subsequent times, the image is already built,\n",
        "# and it will run much faster.\n",
        "#\n",
        "#================================================================================================\n",
        "\n",
        "\n",
        "@app.local_entrypoint()\n",
        "def entrypoint(\n",
        "    samples: int = 4,\n",
        "    prompt: str = \"A princess riding on a pony\",\n",
        "    batch_size: int = 4,\n",
        "    seed: int = None,\n",
        "):\n",
        "    print(\n",
        "        f\"prompt => {prompt}\",\n",
        "        f\"samples => {samples}\",\n",
        "        f\"batch_size => {batch_size}\",\n",
        "        f\"seed => {seed}\",\n",
        "        sep=\"\\n\",\n",
        "    )\n",
        "\n",
        "    output_dir = Path(\"/tmp/stable-diffusion\")\n",
        "    output_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    inference_service = Model()\n",
        "\n",
        "    for sample_idx in range(samples):\n",
        "        start = time.time()\n",
        "        images = inference_service.run.remote(prompt, batch_size, seed)\n",
        "        duration = time.time() - start\n",
        "        print(f\"Run {sample_idx+1} took {duration:.3f}s\")\n",
        "        if sample_idx:\n",
        "            print(\n",
        "                f\"\\tGenerated {len(images)} image(s) at {(duration)/len(images):.3f}s / image.\"\n",
        "            )\n",
        "        for batch_idx, image_bytes in enumerate(images):\n",
        "            output_path = (\n",
        "                output_dir\n",
        "                / f\"output_{slugify(prompt)[:64]}_{str(sample_idx).zfill(2)}_{str(batch_idx).zfill(2)}.png\"\n",
        "            )\n",
        "            if not batch_idx:\n",
        "                print(\"Saving outputs\", end=\"\\n\\t\")\n",
        "            print(\n",
        "                output_path,\n",
        "                end=\"\\n\" + (\"\\t\" if batch_idx < len(images) - 1 else \"\"),\n",
        "            )\n",
        "            output_path.write_bytes(image_bytes)\n",
        "\n",
        "\n",
        "#================================================================================================\n",
        "#\n",
        "# FRONT-END\n",
        "#\n",
        "#================================================================================================\n",
        "\n",
        "\n",
        "frontend_path = Path(__file__).parent / \"frontend\"\n",
        "\n",
        "web_image = (\n",
        "    modal.Image.debian_slim(python_version=\"3.12\")\n",
        "    .pip_install(\"jinja2==3.1.4\", \"fastapi[standard]==0.115.4\")\n",
        "    .add_local_dir(frontend_path, remote_path=\"/assets\")\n",
        ")\n",
        "\n",
        "\n",
        "@app.function(\n",
        "    image=web_image,\n",
        "    allow_concurrent_inputs=1000,\n",
        ")\n",
        "@modal.asgi_app()\n",
        "def ui():\n",
        "    import fastapi.staticfiles\n",
        "    from fastapi import FastAPI, Request\n",
        "    from fastapi.templating import Jinja2Templates\n",
        "\n",
        "    web_app = FastAPI()\n",
        "    templates = Jinja2Templates(directory=\"/assets\")\n",
        "\n",
        "    @web_app.get(\"/\")\n",
        "    async def read_root(request: Request):\n",
        "        return templates.TemplateResponse(\n",
        "            \"index.html\",\n",
        "            {\n",
        "                \"request\": request,\n",
        "                \"inference_url\": Inference.web.web_url,\n",
        "                \"model_name\": \"Stable Diffusion 3.5 Large Turbo\",\n",
        "                \"default_prompt\": \"A cinematic shot of a baby raccoon wearing an intricate italian priest robe.\",\n",
        "            },\n",
        "        )\n",
        "\n",
        "    web_app.mount(\n",
        "        \"/static\",\n",
        "        fastapi.staticfiles.StaticFiles(directory=\"/assets\"),\n",
        "        name=\"static\",\n",
        "    )\n",
        "\n",
        "    return web_app\n",
        "\n",
        "\n",
        "def slugify(s: str) -> str:\n",
        "    return \"\".join(c if c.isalnum() else \"-\" for c in s).strip(\"-\")\n"
      ],
      "metadata": {
        "id": "ZPVCRf32ZwFL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "```"
      ],
      "metadata": {
        "id": "tj-q7U7se4z_"
      }
    }
  ]
}